{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fake Reviewers\n",
    "###  The problem statement\n",
    "They are helpful, but blindly trust the these reviews are dangerous for both the seller and buyer. Hence it is important to identify the reviewers that are generating unfair and wrong assessment regarding a business. In this study a binary classification is performed on raw data collected from Yelp, based on some attributes to identify fake an real reviewers. <br>\n",
    "In the literature, fake reviews are categorized into three groups, proposed by Dixit et al.: (1) Untruthful Reviews (2) Reviews on Brands of where the comments are only concerned with the brand or the seller of the product and fail to review the product, and (3) Non-Reviews where the reviews contain either unrelated text or advertisements. The first category, untruthful reviews, is of most concern as they undermine the integrity of the online review system. Detection of this type 1 review is a challenging task. It is impossible, to distinguish between fake and real reviews by manually\n",
    "reading them. \n",
    "\n",
    "###  Why is data science used to identify fake reviewers\n",
    "As of 2014 there were over 18 million reviews created on Yelp. Online reviews are constantly being generated on various websites across the Internet. Hence, Big Data techniques are needed to address the problem of fake reviewers. Big\n",
    "Data, is often quantified with (1) Volume and scale of the data, (2) velocity or rate at which new data are created and consumed by processing engines, (3) variety of the different formats that data may be stored in, and (4) veracity of the quality level of the data.<br>\n",
    "The volume and velocity of online reviews are noted by merely visiting e-commerce and customer rating sites, such as Yelp and Amazon. Also a great variety of data is possible for industry sectors such as hotels, restaurants, e-commerce,\n",
    "home services, etc. However the vast majority of this dataset is unlabeled, which means it is not easily known whether the review is fake or not. Thus, review spam detection is a Big Data problem, as there are numerous challenges when analyzing and classifying varying reviews from disconnected sources.\n",
    "### What kind of problem is it ?\n",
    "The problem is a binary classification problem (1) Fake reviewer (category 1 in this study) (2) Real reviewer (category 0 in this study). The dataset is hand labeled to fake or real review and is verified from Review Skeptic (http://reviewskeptic.com/). The dataset set consists of 200 samples 80% of which is used for the training set and the rest 20% for the test set. <br>\n",
    "A model is trained on the training set and the parameters generated from the model are then implemented on the test set. These parameters predict how accurately the model performs on the test set. \n",
    "\n",
    "### Data Acquisition and Cleaning\n",
    "The dataset is acquire from Yelp (https://www.yelp.com/dataset). Yelp Dataset JSON contains files composed of a single object type, one JSON-object per-line.\n",
    "<br>\n",
    "business.json : Contains business data including location data, attributes, and categories.\n",
    "<br>\n",
    "review.json: Contains full review text data including the user_id that wrote the review and the business_id the review is written for.\n",
    "<br>\n",
    "user.json: User data including the user's friend mapping and all the metadata associated with the user.\n",
    "<br>\n",
    "checkin.json: Checkins on a business.\n",
    "<br>\n",
    "photo.json: Contains photo data including the caption and classification (one of \"food\", \"drink\", \"menu\", \"inside\" or \"outside\").\n",
    "<br> \n",
    "tip.json: Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "In this study checkin.json, photo.json and tip.json are not used.<br>\n",
    "More than 200 user ids are extracted fron the user dataset (user.json file). These user ids are matched with review.json file to extract review texts. Many of the users have more than one review but only the first review is extracted for every users. In very few cases the user ids do not have a corrosponding review in the review file. Those user ids are not used hence this is a biased sample. More details about acquiring and concatenating the data can be found in this IPython notebook.<br> \n",
    "The diverse array of tags associated with the business reviewed by the fake and real profiles is shown in Figure below. Most of the categories are related to food followed by hotel reviews, automobile reviews and so on. The categories for fake and real profiles almost a balanced distribution of categories. \n",
    "<img src=\"1.png\" width=\"800\" /> ![](1.png)\n",
    "<img src=\"2.png\">\n",
    "### Natural language processing\n",
    "Natural language processing (NLP) involves the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. NLTK is a leading platform for building python programs to work with human language data. Some of the features of NLTK are discussed below.<br>\n",
    "**Tokenization**: In language processing, the string is broken into words and punctuation. This step is called tokenization, and it produces a familiar structure, a list of words and punctuation.<br>\n",
    "**Stop words**:Text may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.<br>\n",
    "**Lemmatization**: Lemmatisation in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatisation is the algorithmic process of determining the lemma for a given word.It requires the knowledge of the grammar of a language.<br>\n",
    "**Part of Speech (POS) tagging**: POS tagging captures the structure of the sentences. The part of speech explains how a word is used in a sentence. There are eight main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections. POS tagging is a supervised learning solution that uses features like the previous word, next word, is first letter capitalized etc. NLTK has a function to get pos tags and it works after tokenization process.<br>\n",
    "###  Characteristics  of feature of raw data \n",
    "Feature engineering transforms raw data into something usable that can used for machine learning. This problem is a mix of categorical feature and cardinal (continuous) features along with text. The problem is tested with categorical feature and continuous features along with basic natural language processing analysis.The next few plots show the probability density distribution of some features.\n",
    "#### Review length\n",
    "The average review length may be an important indication of reviewers with questionable intentions since about 80 % of spammers have no reviews longer than 135 words while more than 92% of reliable reviewers have an average review\n",
    "length of greater than 200 words. The distribution shows that fake reviewers write very short reviews as well as some longer reviews too. Hence, Figure shows a bimodal distribution in comparison to real profiles. The mean of probability density distribution of real reviewers is less than fake reviewers. About 10% of the fake review lengths are greater than 1000 characters.\n",
    "![alt text](3.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "\n",
    "#### Review length without stop word\n",
    "The mean of real and fake reviewers are the same around 400 characters. Also about 10% of the fake review lengths are greater than 1000 characters. Real reviewers use more stop words to express emotions. Since fake reviewers use a more direct approach so do not use much stop words. \n",
    "![alt text](4.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Number of nouns  and verbs in reviews\n",
    "When reviewers want to sound sincere, but aren’t, they use more first-person pronouns like “I” and “me. Genuine reviews focus more heavily on describing situations with nouns, while fake reviews replace these with verbs. This “action” is supposed to make the reviews sound more convincing, but it ends up doing the opposite. The mean number of nouns is 20 for real and 30 for fake which is not very significant difference. In Figure there is a bimodal distribution of verbs for fake reviewers about 15% of the reviewers use 60 verbs.\n",
    "![alt text](5.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "![alt text](6.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Number of friends  and fans\n",
    "This dataset is not very complete and hence shows some fake reviewers are having more fans than real reviewers. But in general fake reviewers have a low social profile.\n",
    "![alt text](7.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of reviews and useful\n",
    "The number of times reviews have been marked useful is shown in the Figure. Fake reviewers write mostly one review compared to real reviewers who write more than one review. Also reviews from real reviewers are marked useful more often compared to fake. The labels used in the graph are described below. \n",
    "\n",
    "label=0 : Only one review.\n",
    "<br>\n",
    "label=1 : More than 1 and upto 20 reviews.\n",
    "<br>\n",
    "label=2 : More than 20 and upto 50 reviews.\n",
    "<br>\n",
    "label=3 : More than 50 and upto 100 reviews.\n",
    "<br>\n",
    "label=4 : More than 100 reviews.\n",
    "![alt text](8.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Distribution of actual bussiness stars and stas rating given by reviewer\n",
    "In case of a real profile 3.5, 4.0, 4.5 and 5 star businesses received ratings more positive ratings than 1 and 2 stars. However, in fake profile 3.5, 4.0 and 4.5 businesses are rated 1 stars by more than 7 fake reviewers. Also fake reviews tend to give more 1 star ratings than real reviewers.\n",
    "![alt text](9.png \"Categories of bussiness reviewed by real reviewers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and evaluation metric\n",
    "Once the data is pre-processed, they are fed to the classification algorithm to build the model. In order to evaluate the performance of the model, the model is tested on the test dataset. Before making predictions on test dataset, we use the exact same pre-processing steps that we used for training dataset and apply them on the test dataset. Python’s scikit learn library are used for the machine learning approaches. The “pipeline\" functions is also used to combine all the steps, i.e. pre-processing and classifier learning steps into one. The 200 reviews are divided into 50% of fake and real\n",
    "reviews. The test-training split is made 20%-80%. So a total of 160 rows of data are used for training set and 40 rows of data used for test set. The fit of the model is tested based on confusion matrix.\n",
    "#### Confusion matrix\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. There are two possible predicted classes: \"yes\" and \"no\".The most basic terms are whole numbers (not rates):\n",
    "- true positives (TP): These are cases in which are predicted yes and are yes. \n",
    "- true negatives (TN): These are cases in which are predicted no and are no. \n",
    "- false positives (FP): These are cases in which are predicted yes but are no.(Also known as a \"Type I error.\")\n",
    "- false negatives (FN): These are cases in which are predicted no but are yes. (Also known as a \"Type II error.\")\n",
    "This is a list of rates that are often computed from a confusion matrix for a binary classifier:\n",
    "- Accuracy: Overall, how often is the classifier correct? $\\frac{(TP+TN)}{total}$. \n",
    "- Precision: When it predicts yes, how often is it correct? $\\frac{TP}{TP+FP}$\n",
    "- Recall: The ability of a model to find all the relevant cases within a dataset. $\\frac{TP}{TP+FN}$\n",
    "- F-Score: This is a weighted average of the true positive rate (recall) and precision. $2* \\frac{Precision*Recall}{Precision+Recall}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning methods:\n",
    "\n",
    "\n",
    "### Logistic Regression:\n",
    "The logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $x$, while at the same time ensuring that they sum to one and remain in $[0,1]$. The model has the form\n",
    "\\begin{equation}\n",
    "log \\frac{Pr(G= K-1|X=x)}{Pr(G= K|X=x)} = \\beta_{(K-1)0} + \\beta_{(K-1)}^T x.\n",
    "\\end{equation}\n",
    "The model is specified in terms of $K-1$ logit transformations (reflecting the constraint that the probabilities sum to one). Although the model uses the last class as the denominator in the odds-ratios, the choice of denominator is arbitrary in that the estimates are equivariant under this choice. Since $Pr(G|X)$ completely specifies the conditional distribution, the multinomial distribution is appropriate.\n",
    "\n",
    "### Naive Bayes:\n",
    "Naive Bayes is a very simple classification algorithm that makes some strong assumptions about the independence of each input variable.There are two types of quantities that need to be calculated from the dataset for the naive Bayes model:\n",
    "Class Probabilities and Conditional Probabilities. The class probabilities for classes 0 and 1 are \n",
    "\\begin{equation}\n",
    "P(class=1) = count(class=1) / (count(class=0) + count(class=1))\n",
    "P(class=0) = count(class=0) / (count(class=0) + count(class=1))\n",
    "\\end{equation}    \n",
    "The conditional probabilities are the probability of each input value given each class value. To make predictions using Bayes Theorem.\n",
    "\\begin{equation}\n",
    "P(h|d) = (P(d|h) * P(h)) / P(d),\n",
    "\\end{equation}\n",
    "where:\n",
    "\n",
    "P(h|d) is the probability of hypothesis h given the data d. This is called the posterior probability.<br>\n",
    "P(d|h) is the probability of data d given that the hypothesis h was true.<br>\n",
    "P(h) is the probability of hypothesis h being true (regardless of the data). This is called the prior probability of h.<br>\n",
    "P(d) is the probability of the data (regardless of the hypothesis).<br>\n",
    "\n",
    "\n",
    "### Ensemble method\n",
    "The idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models. Gradient Bagging and random forests are ensemble methods for classification, where a committee of trees each cast a vote for the predicted class. Stacking is a novel approach to combining the strengths of a number of fitted models. Ensemble learning can be broken down into two tasks: developing a population of base learners from the training data, and then combining them to form the composite predictor.<br>\n",
    "#### Gradient Boosting\n",
    "This is an ensemble technique in which the predictors are not made independently, but sequentially. In this technique the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most (the observations are not chosen based on the bootstrap process, but based on the error). The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data.<br>\n",
    "#### Random Forest\n",
    "The essential idea in bagging is to average many noisy but approximately unbiased models, and hence reduce the variance. Trees are ideal candidates for bagging, since they can capture complex interaction structures in the data, and if grown sufficiently deep, have relatively low bias. Since trees are notoriously noisy, they benefit greatly from the averaging. Moreover, since each tree generated in bagging is identically distributed, the expectation of an average of B such trees is the same as the expectation of any one of them. The random forest algorithm is defined as an average of B identically distributed random variables, each with variance $\\sigma^2$, has variance $\\frac{1}{B} \\sigma^2$. If the variables are simply identically distributed, but not necessarily independent) with positive pairwise correlation $\\rho$, the variance of the average is $\\rho \\sigma + \\frac{1-\\rho}{B}\\sigma^2$. The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables.<br>\n",
    "##### Feature Selection\n",
    "The model has 43 hyperparameters to be optimized. A feature selection method is used as shown in Figure to find the most important features. The top 20 features in terms of their feature importances are shown in the figure. Out of these top 20 features, 13 have information about the review text, 2 are information on business and 5 have information on reviewer characteristics.\n",
    "![alt text](15.png \"Categories of bussiness reviewed by real reviewers\")\n",
    "#### Extra-Tree\n",
    "Extra-Trees algorithm builds an ensemble of unpruned decision or regression trees according to the classical top-down procedure. Its two main differences with other treebased ensemble methods are that it splits nodes by choosing cut-points fully at random and that it uses the whole learning sample (rather than a bootstrap replica) to grow the trees. The predictions of the trees are aggregated to yield the final prediction, by majority vote in classification problems and arithmetic average in regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tunning in machine learning models\n",
    "This section disusses chossing the best parameter for the machine learning models based on accuracy score.\n",
    "#### Inverse of regulization: logistic regression:\n",
    "An inverse of regulization is used to improve the generalization performance, i.e., the performance on new, unseen data. In more specific terms, regularization is increasing bias if our model suffers from (high) varying (i.e., it overfits the training data). On the other hand, too much bias will result in underfitting. The figure shows the accuracy score for a set of inverse of regulization values. 1, 10 and 100 have almost same accuracy score so a value of 100 is chosen to avoid underfitting.\n",
    "![alt text](11.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of steps: gradient boosting\n",
    "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Figure shows that 30 or 40 steps gives the maximum accuracy.\n",
    "![alt text](12.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of trees: random forest\n",
    "The number of trees in the forest. Figure shows that 70 or 100 trees gives maximum accuracy.\n",
    "![alt text](14.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "#### Number of trees: extra tree\n",
    "The number of trees in the forest. Figure shows that 70 gives the best estimate of maximum accuracy.\n",
    "![alt text](13.png \"Categories of bussiness reviewed by fake reviewers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons\n",
    "Logistic Regression, Gaussian Naive Bayes, Random Forest, Gradient Boosting and Extra Randomized Trees classifiers to build a model to predict Fake reviewers. Results are shown for both the training set and testing set. The results\n",
    "of various evaluation metrics scores are shown in Tabel1 for all models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model | Accuracy of training set| Accuracy of test set|Precision of training set| Precision of test set|\n",
    "---|---|---|---|---|\n",
    "<span style=\"color:green\">**Logistic Regression**|0.68|0.70|0.68|0.79\n",
    "Naive Bayes|0.56|0.5|0.59|0.5|\n",
    "Gradient Boosting|0.69|\t0.63|0.70|0.67|\n",
    "Random Forest|0.94|\t0.73|0.95|0.73|\n",
    "Extra Tree|0.97|0.68|0.97|0.67|\n",
    "Table 1: Comparing all models for train-test set. Logistic regression (green) is the best model that predicts with an accuracy of 70% and precision of 79%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Recommendations\n",
    "This section discusses the best model that can be used to predict fake reviewers. In statistics, a fit refers to how well  a target function is approximated. In supervised machine learning the unknown underlying mapping function is approximated from the output variables of the training set. The target function is not always known so calculating the residual errors is not always the case in machine learning. In supervised learning approximations are made from samples of noisy training data. <br>\n",
    "Two scenarios appear (1) Overfitting and (2) Underfitting. Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. Decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.<br>\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. A good fit in Machine Learning happends when the model is at the sweet spot between underfitting and overfitting which is very difficult to do in practice.<br>\n",
    "Table 1 shows accuracy and precision for the five different models used in the study. The extra tree and random forest perform best on the training set 97% and 94% respectively. Logistic regression has an accuracy of 68%, whereas gradient boosting and Naive Bayes doesn't perform well. The accuracy and prediction on test set has a diffrent story. In test set data the best accuracy is predicted by random forest which is 73% followed by logistic regression of 70%. However, it is to be noted that random forest suffers from overfitting i.e a vast difference in accuracy between test and training set. The decision trees have trained on noise so the accuracy has decreased from 94% on training set to 73% on test set. Overfitting is also noted in extra tree where the accuracy has decreased from 97% on training set to 68% on test set. The logistic regression model on the other hand has almost similar accuracies of 68% and 70% on the train and test set respetively. The logistic regression also have high precision of 79% i.e. it can predict the positives in the model quite well. In this particular problem logistic regression is a good fit model performing with an accuracy of 70% on test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, The review text of users were merged with bussiness reviewed by the user. 200 review samples were labelled as fake and real. A quick summary of the exploratory data analysis reveals that 1. Fake reviewers use more verbs and nouns than real user. 2. Fake reviewers use less stop words.i.e express less emotions. 3. Fake reviewers give more 1 star reviews than real users. 4. Reviews of real users are more useful. 5. There is a disparity in star rating on bussiness and star rating by fake users. <br>\n",
    "After exploring all datasets, we used five different supervised classification algorithms (Logistic Regression, Gaussian Naive Bayes, Random Forest, Gradient Boosting and Extremely Randomized Trees) are used to train the predictive model by using 80% of the whole data. The remaining 20% was used to evaluate the model ie test data. Overall logistic regression performed well on both test and train data set with 70% accuracy. Random forest and extra tree performed well on training dataset but suffered from overfitting on test data set.<br>\n",
    "However, it is to be noted that there are some limitations in the current model such as lack of complete data because only 200 samples were labelled for fake and real. It is also difficult to identify fake and real charecteretstics.  The model can be furthur improved in future with more sampled dataset and some reviews that are already marked as fake and real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
